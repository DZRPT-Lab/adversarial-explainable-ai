# [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome) Adversarial Interpretable Machine Learning

A curated list of awesome adversarial interpretable machine learning resources, inspired by
[awesome-adversarial-machine-learning](https://github.com/yenchenlin/awesome-adversarial-machine-learning) and
[awesome-interpretable-machine-learning](https://github.com/lopusz/awesome-interpretable-machine-learning).
Due to the novelty of the field, this list is very much in the making. Contributions are welcome - send a pull request
or contact me [@hbaniecki](https://github.com/hbaniecki#:~:text=hbaniecki@gmail.com).

<p align="center"><img src="fig/aiml.png"></p>

There are various adversarial attacks on machine learning models; hence, ways of defending, e.g. by using model explanations. Nowadays, attacks on interpretable machine learning techniques come to light, so does the defense to such adversary. *Veritas Vincit*

## Papers

### General

* [Towards Robust Interpretability with Self-Explaining Neural Networks](https://papers.nips.cc/paper/8003-towards-robust-interpretability-with-self-explaining-neural-networks#:~:text=pdf)
  * D. A. Melis & T. Jaakkola *Advances in Neural Information Processing Systems* 2018
* [Sanity Checks for Saliency Maps](https://papers.nips.cc/paper/8160-sanity-checks-for-saliency-maps#:~:text=pdf#:~:text=pdf)
  * A. Julius et al. *Advances in Neural Information Processing Systems* 2018
* [On Relating Explanations and Adversarial Examples](https://papers.nips.cc/paper/9717-on-relating-explanations-and-adversarial-examples#:~:text=pdf)
  * A. Ignatiev et al. *Advances in Neural Information Processing Systems* 2019
* [Robustness in Machine Learning Explanations: Does It Matter?](https://www.dropbox.com/s/u4kwdk9m5o2u2sb/preprint.pdf)
  * H. L. Leif *Conference on Fairness, Accountability, and Transparency* 2020

### Attack

* [Interpretation of Neural Networks Is Fragile](https://www.aaai.org/ojs/index.php/AAAI/article/view/4252#:~:text=pdf)
  * A. Ghorbani et al. *AAAI Conference on Artificial Intelligence* 2019
* [Fairwashing: the risk of rationalization](http://proceedings.mlr.press/v97/aivodji19a#:~:text=pdf)
  * U. Aivodji et al. *International Conference on Machine Learning* 2019
* [The (Un)reliability of Saliency Methods](https://www.researchgate.net/publication/335707891_The_Unreliability_of_Saliency_Methods#:~:text=Public)
  * P. J. Kindermans et al. *Explainable AI: Interpreting, Explaining and Visualizing Deep Learning* 2019
* [Fooling Neural Network Interpretations via Adversarial Model Manipulation](http://papers.nips.cc/paper/8558-fooling-neural-network-interpretations-via-adversarial-model-manipulation#:~:text=pdf)
  * J. Heo et al. *Advances in Neural Information Processing Systems* 2019
* [Explanations can be manipulated and geometry is to blame](https://papers.nips.cc/paper/9511-explanations-can-be-manipulated-and-geometry-is-to-blame#:~:text=pdf)
  * A. K. Dombrowski et al. *Advances in Neural Information Processing Systems* 2019 <p align="center"><img height='350' src="fig/attack.png"></p>
* [You Shouldn't Trust Me: Learning Models Which Conceal Unfairness From Multiple Explanation Methods](http://ecai2020.eu/papers/72_paper.pdf)
  * B. Dimanov et al. *European Conference on Artificial Intelligence* 2020
* [Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods](https://dl.acm.org/doi/10.1145/3375627.3375830#:~:text=pdf)
  * D. Slack et al. *AAAI/ACM Conference on AI, Ethics, and Society* 2020
* [Faking Fairness via Stealthily Biased Sampling](https://www.aaai.org/ojs/index.php/AAAI/article/view/5377#:~:text=pdf)
  * K. Fukuchi et al. *AAAI Conference on Artificial Intelligence* 2020
* [Interpretable Deep Learning under Fire](https://www.usenix.org/conference/usenixsecurity20/presentation/zhang-xinyang#:~:text=pdf)
  * X. Zhang et al. *USENIX Security Symposium* 2020
* [The Bouncer Problem: Challenges to Remote Explainability](https://arxiv.org/abs/1910.01432#:~:text=pdf)
  * E. L. Merrer & G. Tredan *arXiv preprint* 2020

### Defense

* [Adversarial Explanations for Understanding Image Classification Decisions and Improved NN Robustness](https://arxiv.org/abs/1906.02896#:~:text=pdf)
  * W. Woods et al. *Nature Machine Intelligence* 2019
* [On the (In)fidelity and Sensitivity of Explanations](http://papers.nips.cc/paper/9278-on-the-infidelity-and-sensitivity-of-explanations#:~:text=pdf)
  * C. K. Yeh et al. *Advances in Neural Information Processing Systems* 2019
* [A simple defense against adversarial attacks on heatmap explanations](https://arxiv.org/abs/2007.06381#:~:text=pdf)
  * L. Rieger & L. K. Hansen *ICML Workshop on Human Interpretability in Machine Learning* 2020 <p align="center"><img height='350' src="fig/defense.png"></p>
* [Proper Network Interpretability Helps Adversarial Robustness in Classification](https://proceedings.icml.cc/static/paper_files/icml/2020/1661-Paper.pdf)
  * A. Boopathy et al. *International Conference on Machine Learning* 2020
* [Aggregating explanation methods for stable and robust explainability](https://arxiv.org/abs/1903.00519#:~:text=pdf)
  * L. Rieger & L. K. Hansen *arXiv preprint* 2020

### Other

* [Evaluating Explanation Methods for Deep Learning in Security](https://arxiv.org/abs/1906.02108#:~:text=pdf)
  * A. Warnecke et al. *IEEE European Symposium on Security and Privacy* 2020
* [Evaluating and Aggregating Feature-based Model Explanations](https://arxiv.org/abs/2005.00631#:~:text=pdf)
  * U. Bhatt et al. *arXiv preprint* 2020
